{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "da1c4ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentiment': 'Negative', 'summary': \"The device's photo quality is excellent with the camera performing well in both daylight and night. The processor is the fastest I've ever used. However, UI needs optimization: apps like Facebook and Instagram are slow, there's freezing when switching apps and videos lag.\"}\n"
     ]
    }
   ],
   "source": [
    "# WITH STRUCTURED OUTPUT and with classes for structured output\n",
    "from langchain_groq import ChatGroq\n",
    "from typing import TypedDict               ## Using Typedict first \n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "llm=ChatGroq(\n",
    "    model=\"deepseek-r1-distill-llama-70b\",\n",
    "    temperature=1.5,\n",
    "    max_tokens=None,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "# we dont need to write the prompt that you are and AI agent etc. etc., it is done by itself using TypedDict\n",
    "class Review(TypedDict):\n",
    "    summary:str\n",
    "    sentiment:str\n",
    "\n",
    "structured_llm=llm.with_structured_output(Review)\n",
    "\n",
    "result=structured_llm.invoke(\"\"\" The hardware is great, but the software feels bloated.\n",
    "There are roo many pre-installed apps that I can't remove\n",
    "Also the UI is not looking good, can you fix it\"\"\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f7a29d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentiment': 'bad', 'summary': 'The hardware is great, but the software feels bloated and lacks visual appeal. There are too many pre-installed apps that cannot be removed, which is inconvenient. The UI needs improvement.'}\n"
     ]
    }
   ],
   "source": [
    "# WITH STRUCTURED OUTPUT and with classes for structured output\n",
    "from langchain_groq import ChatGroq\n",
    "from typing import TypedDict,Annotated               ## Using Typedict first \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "llm=ChatGroq(\n",
    "    model=\"deepseek-r1-distill-llama-70b\",\n",
    "    temperature=1.5,\n",
    "    max_tokens=None,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "# Using Annotated, we can define what we exactly need, because we dont want our llm to hallucinate or give wrong outputs\n",
    "\n",
    "class Review(TypedDict):\n",
    "    summary:Annotated[str,\"A brief summary of the review\"]\n",
    "    sentiment:Annotated[str,\"Sentiment of the review, either good, bad or neutral\"]\n",
    "\n",
    "structured_llm=llm.with_structured_output(Review)\n",
    "\n",
    "result=structured_llm.invoke(\"\"\" The hardware is great, but the software feels bloated.\n",
    "There are roo many pre-installed apps that I can't remove\n",
    "Also the UI is not looking good, can you fix it\"\"\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dc2eef4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cons': ['The software feels bloated', 'Too many pre-installed apps', 'UI is not visually appealing', 'UI is difficult to navigate'], 'key_themes': ['Hardware quality', 'Software issues', 'UI/UX problems'], 'name': 'SID', 'pros': ['The hardware is great'], 'sentiment': 'neutral', 'summary': \"The product's hardware is excellent, but the software has some issues.\"}\n",
      "neutral\n",
      "SID\n"
     ]
    }
   ],
   "source": [
    "# WITH STRUCTURED OUTPUT and with classes for structured output\n",
    "from langchain_groq import ChatGroq\n",
    "from typing import TypedDict,Annotated,Optional,Literal              ## Using Typedict first \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "llm=ChatGroq(\n",
    "    model=\"deepseek-r1-distill-llama-70b\",\n",
    "    temperature=1.5,\n",
    "    max_tokens=None,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "# Using Annotated, we can define what we exactly need, because we dont want our llm to hallucinate or give wrong outputs\n",
    "\n",
    "class Review(TypedDict):\n",
    "    key_themes:Annotated[list[str],\"Write down all the key themes discussed in the review in a list\"]\n",
    "    summary:Annotated[str,\"A brief summary of the review\"]\n",
    "    sentiment:Annotated[str,\"Sentiment of the review, either good, bad or neutral\"] # Both are correct, one returns the string and other returns the literal\n",
    "    # sentiment:Annotated[Literal[\"pos\",\"neg\"],\"Return the sentiment of the review\"]\n",
    "    pros:Annotated[Optional[list[str]], \"Write down all the pros in the list\"] # These are optional fields i.e. it can be present or absent\n",
    "    cons:Annotated[Optional[list[str]], \"Write down all the cons in the list\"] # These are optional fields i.e. it can be present or absent\n",
    "    name:Annotated[Optional[str],\"Write down the name of the reviewer\"] # Since name is not present in the paragraph so it will not display name\n",
    "\n",
    "structured_llm=llm.with_structured_output(Review)\n",
    "\n",
    "result=structured_llm.invoke(\"\"\" The hardware is great, but the software feels bloated.\n",
    "There are roo many pre-installed apps that I can't remove\n",
    "Also the UI is not looking good, can you fix it\n",
    "Reviewed by SID \"\"\")\n",
    "\n",
    "print(result)\n",
    "print(result['sentiment'])\n",
    "print(result['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "769af6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='SID' age=30 email='abc@gmail.com' cgpa=5.0\n",
      "<class '__main__.Student'>\n",
      "30\n",
      "{\"name\":\"SID\",\"age\":30,\"email\":\"abc@gmail.com\",\"cgpa\":5.0}\n"
     ]
    }
   ],
   "source": [
    "### Pydantic demo\n",
    "# ## Pydantic is a data validation and data parsing library for Python. It ensures that the data you work with is correct, structured and type-safe\n",
    "\n",
    "from pydantic import BaseModel, EmailStr, Field\n",
    "from typing import Optional\n",
    "# class Student(BaseModel):\n",
    "#     name:str\n",
    "\n",
    "# new_student={'name':'sid'}\n",
    "\n",
    "######## If you want to pass a default and Optional value. #########\n",
    "\n",
    "class Student(BaseModel):\n",
    "    name: str = 'SID'        # It is default now\n",
    "    age: Optional[int] = None\n",
    "    email: EmailStr\n",
    "    cgpa: float=Field(gt=0,lt=10)\n",
    "new_student={'age':30 , 'email':'abc@gmail.com','cgpa':5}      # Even if you pass the age in str and it is a valid number the pydantic is smart enough to convert it into int and so we do not get any err.\n",
    "\n",
    "student= Student(**new_student)\n",
    "print(student)\n",
    "print(type(student))\n",
    "\n",
    "student_dict=dict(student)\n",
    "print(student_dict['age'])\n",
    "\n",
    "student_json=student.model_dump_json()\n",
    "print(student_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "10c56bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key_themes=['Software performance', 'Bloatware', 'Poor UI design'] summary=['The hardware is great but the software feels bloated and has too many pre-installed apps, plus the UI is not looking good.'] sentiment='neg' pros=['hardware is great'] cons=['software feels bloated', 'too many pre-installed apps', 'UI is not looking good'] name='SID'\n",
      "SID\n"
     ]
    }
   ],
   "source": [
    "# WITH STRUCTURED OUTPUT and with classes for structured output\n",
    "# now with Pydantic instead to Typedict\n",
    "from langchain_groq import ChatGroq\n",
    "from pydantic import BaseModel, Field\n",
    "# from typing import TypedDict,Annotated,Optional,Literal              ## Using Typedict first \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "llm=ChatGroq(\n",
    "    model=\"deepseek-r1-distill-llama-70b\",\n",
    "    temperature=1.5,\n",
    "    max_tokens=None,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "# Using Annotated, we can define what we exactly need, because we dont want our llm to hallucinate or give wrong outputs\n",
    "\n",
    "class Review(BaseModel):\n",
    "\n",
    "    key_themes:list[str]=Field(description=\"Write down all the key themes discussed in the review in a list\")\n",
    "    summary:list[str]=Field(description=\"A brief summary of the review\")\n",
    "    sentiment:Literal[\"pos\",\"neg\",\"neutral\"]=Field(description=\"Sentiment of the review, either good, bad or neutral\")\n",
    "    pros: Optional[list[str]]=Field(default=None, description=\"Write down all the pros in the list\")\n",
    "    cons: Optional[list[str]]=Field(default=None,description=\"Write down all the cons in the list\")\n",
    "    name: Optional[str]=Field(default=None,description=\"Write down the name of the reviewer\")\n",
    "\n",
    "structured_llm=llm.with_structured_output(Review)\n",
    "\n",
    "result= structured_llm.invoke(\"\"\" The hardware is great, but the software feels bloated. There are roo many pre-installed apps that I can't remove Also the UI is not looking good, can you fix it Reviewed by SID \"\"\")\n",
    "\n",
    "print(result)\n",
    "print(result.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5722ce4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "###### NOW FOR JSON SCHEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89718af4",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#### What is the difference between Structured output and Output parser\n",
    "###  Structured output is an integrated form i.e. an ai model can have integrated structured output and we can use it so that the output is structured while if we use output parsers, whether the llm model supports the structured output or not, we can use it to get the structured output. We can also use the parsers to fetch info of one to other eg->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4ed9dcc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Black holes are fascinating cosmic phenomena formed from the collapse of massive stars, characterized by intense gravity, an event horizon, and a singularity. They exist in various types, including stellar, supermassive, and rotating black holes, each with unique properties. Their effects on spacetime, such as frame-dragging and gravitational waves, are significant and detectable through electromagnetic radiation and advanced observatories. Beyond their scientific importance, black holes inspire cultural and philosophical thought, challenging our understanding of the universe. This report underscores their significance in advancing astrophysical knowledge and unraveling cosmic mysteries.\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "load_dotenv()\n",
    "\n",
    "llm=ChatGroq(\n",
    "    model=\"deepseek-r1-distill-llama-70b\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "template1= PromptTemplate(\n",
    "    template='Write a detailed report on {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "template2= PromptTemplate(\n",
    "    template='Write a short 5 line summary on the following text. /n {text}',\n",
    "    input_variables=['text']\n",
    ")\n",
    "\n",
    "# Either you can use this \n",
    "\n",
    "# prompt1=template1.invoke({'topic':'black hole'})\n",
    "# result1=llm.invoke(prompt1)\n",
    "# prompt2=template2.invoke({'text':result1.content})\n",
    "# result2=llm.invoke(prompt2)\n",
    "# print(result2.content)\n",
    "\n",
    "# OR\n",
    "parser=StrOutputParser()\n",
    "chain = template1 | llm | parser | template2 | llm | parser\n",
    "print(chain.invoke({'topic':'black hole'}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421d2cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Ethan Carter', 'age': 32, 'city': 'New York City'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "load_dotenv()\n",
    "\n",
    "llm=ChatGroq(\n",
    "    model=\"deepseek-r1-distill-llama-70b\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "template=PromptTemplate(\n",
    "    template='Give me the name, age and city of a fictional person \\n {format_instruction}',\n",
    "    input_variables=[],\n",
    "    partial_variables={'format_instruction':parser.get_format_instructions()},\n",
    "\n",
    ")\n",
    "# prompt =template.format()\n",
    "# result= llm.invoke(prompt)\n",
    "# print(result.content)\n",
    "# final_result=parser.parse(result.content)\n",
    "# print(final_result)\n",
    "#  or you can use\n",
    "\n",
    "chain = template | llm | parser\n",
    "result=chain.invoke({}) # you have to send a blank dict\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ede2a007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fact_1': 'Black holes are regions in space where the gravitational pull is so strong that nothing, including light, can escape once it falls inside the event horizon.', 'fact_2': 'There are four types of black holes: stellar black holes, supermassive black holes, intermediate-mass black holes, and miniature black holes.', 'fact_3': 'Black holes are not visible, but their presence can be inferred by the effects they have on nearby stars, gas, and light, such as bending the light around them through gravitational lensing.'}\n"
     ]
    }
   ],
   "source": [
    "### NOW STRUCTURED OUTPUT PARSERS\n",
    "#### USING THIS WE CAN ENFORCE OUR DESIRED SCHEMA TO THE LLM\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "load_dotenv()\n",
    "\n",
    "llm=ChatGroq(\n",
    "    model=\"deepseek-r1-distill-llama-70b\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "schema= [\n",
    "    ResponseSchema(name='fact_1',description='Fact 1 about the topic'),\n",
    "    ResponseSchema(name='fact_2',description='Fact 2 about the topic'),\n",
    "    ResponseSchema(name='fact_3',description='Fact 3 about the topic')\n",
    "]\n",
    "parser =StructuredOutputParser.from_response_schemas(schema)\n",
    "\n",
    "template = PromptTemplate(\n",
    "    template='Give 3 facts about {topic} \\n {format}',\n",
    "    input_variables=['topic'],\n",
    "    partial_variables={'format':parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# prompt=template.invoke({'topic':'black hole'})\n",
    "# result= llm.invoke(prompt)\n",
    "# final_result=parser.parse(result.content)\n",
    "# print(final_result)\n",
    "\n",
    "\n",
    "\n",
    "chain = template | llm | parser\n",
    "result= chain.invoke({'topic':'Black hole'})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1e81e43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Rahul Sharma' age=32 city='Bangalore'\n"
     ]
    }
   ],
   "source": [
    "##### Pydantic Output Parser\n",
    "### We can enforce schema as well as can validate the data which can't be done in structured output parsers\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "load_dotenv()\n",
    "\n",
    "llm=ChatGroq(\n",
    "    model=\"deepseek-r1-distill-llama-70b\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "class Person(BaseModel):\n",
    "    name:str=Field(description='Name of the person')\n",
    "    age: int=Field(description='Age of the person')\n",
    "    city:str=Field(description='Name of the city in which the person belongs')\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Person)\n",
    "template=PromptTemplate(\n",
    "    template='Generate the name, age and city of a fictional {place} person \\n {format}',\n",
    "    input_variables=['place'],\n",
    "    partial_variables={'format':parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# prompt=template.invoke({'place':'indian'})\n",
    "# result=llm.invoke(prompt)\n",
    "# final_result=parser.parse(result.content)\n",
    "# print(final_result)\n",
    "\n",
    "chain =template |llm|parser\n",
    "print(chain.invoke({'place':'India'}))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
